###

NOTES

Compilers are Databases

	Should I structure this compiler like Odersky's core ER (Entity Relationship) diagram?
	Yes, that seems like a good idea.

	Should I literally build tables of entities here?
	No. But something more flexible than what Martin did is a good idea.

	Is the flexibility issue I'm seeing an interface problem?
	Yes.

	How do I do interfaces better?
	Just think - ...

What's a Type?

	A type is what we know (or assume) about something.

	A type may be associated with a tree (but it doesn't have to be).
	A type may be associated with a symbol (but it doesn't have to be).

	So what does a type do?

	What do we want to do with a type?

	We want to make sure that for every term selection p.x, p contains x.
	We want to make sure that for every type selection p.T, p contains T.
	We want to make sure that for every term assignment a = b, b is substitutable for a.
	We want to make sure that for every term assignment a = b, a is assignable.
	(Assignability is [among other things] related to variance and privacy: a field is not writable/assignable if its type is covariant and non-private [which is notably the case for all fields in DOT].)
	We want to make sure that for every instance creation T.new, all needed members of T are known.
	For every declaration block '{ stmts }', we want to know what declarations it contains.
	The type of a statement block must be related to: 1. The declarations in that block (which may be inferred), and 2. the sequence of assignments in that block (term and type assignments).

	What about subtyping?
	'{ stmts1 }' <: '{ stmts2 }' if for every declaration in stmts2 there is a compatible declaration in stmts1. That is, for all D2 in stmts2, there exists a D1 in stmts1 s.t. D1 <: D2. This interpretation of statement-block subtyping is compatible with the idea that the type of a statement block is the intersection of all declarations in that block.


	Declaration subsumption
	We only have 2 kinds of declarations.
	Type declarations have the form 'ID : A .. B' where A and B are types.
	Term declarations have the form 'id : T' where T is a type.

	'id1:T1' <: 'id2:T2' if T1 == T2. (May be relaxed if T1 or T2 is covariant or contravariant.)

	'ID1:A1..B1 <: ID2:A2..B2' if A2 <: A1 and B1 <: B2.


	Inheritance of Field Types
	Despite DOT, it seems like field types should be invariant under inheritance, unless stated otherwise.
	What does this mean for subtyping?

	Scala has separate notions of a getter and setter.
	Say, for example, A <: B. And:
		trait D { var f: A }
		trait E extends D { var f: B = ??? }
	If we try
		new E{}
	Scala won't take it because the setter for f:A has not been defined.
	The getter for f:A is overridden by f:B, so that part's OK.

	Maybe we need to find out how getters and setters work in DOTjs:
	{
		f: A
		GetF = {
			out value: A
			value = f
		}
		SetF = {
			in value: A
			f = value
		}
	}
	Calling:
	{
		fOut = GetF.new.value
		SetF{ value = fIn }.new
	}
	Mixing in:
	{
		f: B
		GetF = {  // presence of public getter constrains f's type <: A&B
			out value: B  // "out" means there's no way to assign outside of this object*
			value = f
		}
		SetF = {  // presence of public setter constrains A|B <: f's type
			in value: B   // "in" means there's no way to read outside of this object**
			f = value
		}
	}
	* Assignments to the value can occur within the statement block where the value is declared.
	  Field f can be arbitrarily assigned within the block because mixin composition occurs
	  over such assignments, and they can be type-checked at the time of object instantiation.
	** Reads of the value can occur within the statement block where the value is declared.
	  Like assignments, reads of values within the statement block are type-checked at the
	  point of object creation. Access to the final type of the field allows this.


	Mixins, declarations, and assignments

	Declarations are order-agnostic. Regardless of mixin ordering, all declarations must be satisfied.
	Satisfaction of declarations means:
		For every declared type T, the upper bound must be no less than the lower bound.
		All assignments to T must be within those bounds.
		For every publically-readable field fo declared with type FO, FO serves as an upper bound
		of the actual type of fo. For every publically-writable field fi with type FI, FI serves
		as a lower bound of the actual type of fi.

	Assignments are order-dependent.

	Type assignments must be evaluated at compile time.
	Term assignments may be evaluated at runtime.

	A type assignment remains in effect only until the type is re-assigned
	(a type assignment is a temporary narrowing of type bounds).

	Here's the key:
	Declarations participate in the static type of an object, but assignments do not.
	(Although it is possible to infer certain declarations given a set of assignments.)

	Example of type assignment:
	J = {
		L = Int
		f: L
		f = 3
		L = String  // changes the type of L for subsequent statements
		g: L
		g = "Hi!"
	}
	j = J.new
	l: j.L   // j.L is a string here, since that was the last assignment to L in j
	l = "Hi again!"

	Key note: (
		All term members start as undefined, all type members as Undefined.
		When are errors generated on use of undefined or Undefined?
	)

	Path-dependent subtyping rules allow j.L to be equal to j.L,
	but another path k.L is not necessarily equal to j.L.

(Triple venti vanilla frappucino)


###

###
APPROACH TO TYPE CHECKING

A function to get the bounds on a tree's type. (What's the result here?)

A function to get the declarations on a tree's type. There are lower- and upper-bound versions.

A function to get the assignments on a tree's type. Ther result is always a constructible lower bound.


HOW TO TYPECHECK: A & { x = y } . new

Register a new context for A & { x = y }    # reflects creation of new object
Linearize statements in A & { x = y }
Resolve and check explicit declarations in A & { x = y }
	(the declarations in A are thereby entered into the newly-created current context.)
Generate type aliases from type assignments (and check that they are within declared bounds).
Infer field types (term declarations) from term assignments (where needed).

Specific to { x = y }:
	Check that y.type <: x.widen.
	Which involves:
		Look up y in the context where y is used (in this case, the newly-created current context)
		Look up x in the context where x is used (in this case, the newly-created current context)
Generalization:
	Check that term assignments have compatible types.
	The type of a path p is simply p itself.
	If necessary, p is widened to its declared type.

Path-dependence and A:
	( assume A is { y: Y;  y = a } )
	Lookup of y from { x = y } produces type Y, which may be in A's outer context.
	( assume { Y: at least L at most U ; A = { y: Y;  y = a } } )
	The type of p.y in "p = A & { x = y } . new" is: p.Y
		because Y is invariant with respect to a particular object reference p.
	So Y without a prefix is always equal to Y without a prefix, and p.Y === p.Y.
		But: Y != p.Y and p.Y != q.Y  ---> path-dependence.
		(unless Y is aliased to another type which does compare.)

Code Generation

{
	A : at least {
		y: Y
		y = ???
	}

	bar = A & { x = y }.new
	bar.x
}

(function (c0) {
	c0.A = function (c1) {
		c1.y = throw "Not Implemented";
	};
	c0.bar = (function (c2) {
		c0.A(c2);
		c2.x = c2.y;
		return c2;   // return is only generated for ".new"
	})({});
	c0.bar.x;
	return c0;
})({});

###

###
Progress Notes, June 14, 2016.

	It seems that I should do an overhaul of how contexts are generated and used.
Specifically, contexts should be given unique names. All user-specified names without
prefixes are prefixed with the appropriate context name.

	My previous approach is probably incorrect in another way also. Previously, I was
assuming that all statements in all constructors could be simply linearized upon
instantiation. However, that approach does not account for the possibility that
constructors will access names in their enclosing contexts. Instead, I now think that
the linearization really needs to identify not merely inherited statements, but rather
the appropriate base-class constructors. These constructors ought to be called as
functions in linearization order.

	Javascript will handle referencing enclosing contexts via is closure mechanism.

Notes, June 15

	Name mangling: If I want to support custom/hidden/private(?) attributes,
I can use a name starting with a low-frequency letter (e.g., "q"). Some escape pattern
can be used for any user names that happen to start with that letter.

	For private fields, perhaps codegen can produce Javascript variables. The closure
mechanism will still be able to access those variables, but they will never be present
within derived types. 

	How do I proceed?
	1. Codegen. Write it out.
	2. ...

Notes, June 17

	On membership:
	The reasons I need to know about membership are:
	1. To determine whether a name selection is always valid (either in the current context, constructor context, or prefix context),
	2. To find out which constructors should be called on named type instantiation.

	Abstract execution plan:
	1. Generate code for constructors. Which depends on:
		a. Finding a sequence of base trees.
		a1. Looking up base trees from named types in the current or enclosing contexts.
			b1. Name declarations must be registered in their current contexts.
			b2. Looking up declarations in a constructor context must search base trees for matching declarations.
				c1. 
		a2. 


June 19.

	Due to the recurive nature of path-dependent typing, I am now skeptical that a simple bottom-up approach to typing
and linearization will work. Finding the complete membership of a type involves finding membership of its term prefix,
which involves finding the membership of that term's type.

	First, we need to be able to move from contexts to trees. This will allow examination of type trees to determine
members, base types, etc.

	Second, we may need a findMember that searches type trees for specific named members. The inputs to findMember are
a prefix type tree and a name. The result of findMember is a type tree. To support unions and intersections, synthetic type trees
may be created.*

	Third, we need a findMember that searches within a context. If searching a constructor context, we switch to the
findMember that searches through type trees.

*There is a canonical form for any type tree: (S1 & ... & SN) | ... | (T1 & ... & TM) where every Sn and Tm is a
statement block.

June 21.

	How do we generate mixin constructors?

	Say we have:
		T : { a } & { b }
	When we go to construct an instance of T:
		T.new
	we need to have a mixin constructor for T that builds an object with a type compatiable with T.
	Compatibility with T means that the constructed object's type must be compatible with T's lower bound.

	As for the statments that go into T's mixin constructor, we can select any statements we want.
	However, we should at least make an effort to ensure that all fields declared in T's lower bound get initialized
	and initialized in a reasonable order, even if the lower bound of T contains union types.
	The solution here is to traverse the lower-bound type of T all the way down to concrete statements,
	and generate all term-like statements in the order that they appear.
	For consistency, we may say that Nothing contains no statements (although it contains all possible
	declarations). By saying that Nothing contains no statements, the lower bound of an intersection type
	A & B -- although quite possibly containing a union with Nothing -- will still be constructible and
	generate all expected statements. There is still an issue with implementation inheritance, however:
	after narrowing a type member's bounds, one would expect that the statements in a newly-declared
	lower bound would supercede -- not append to -- statements in the previously-declared lower bound.

	The issue of implementation inheritance would benefit from some more thought...

	For now, it suffices to say that side-effect-free constructors called in linearization order should
	produce a reasonable result; term-member initializations override any prior conflicting initializations
	at runtime, and indeed unneeded prior initializations are potentially elidable through optimization.

	Perhaps the best thing to do here is try some examples after codegen is completed.


	Another issue is what to do about out-of-order statements. For now, nothing. Ideally, statements
would be reordered based on depenency. But this doesn't have to happen yet.


June 24.

	I may need to re-think exactly how/when symbols are added to contexts. There are problems with circular references in symbol lookups.

	1. Remove contexts on STATEMENTS trees, leaving only constructor/mixin-constructor contexts.
	2. Constructor membership is evaluated lazily. Basically, requesting a member of a context forces
	   computation of the entire memberhsip set for that context. For each constructor, the membership
	   set is always computed before any element is selected from it.

	Possible way to break this approach:
		x: L
		L: x.K   // the membership of L depends on a member of L. It's probably OK to treat this as an error

	Possible counter-counter example:
	L: {
		H: at most L   // OK. The mixin context surrounding L here is evaluated lazily, so all members of L are established before we get here.
	}

July 2.

	The problem of symbol lookup seems to be solved. Up next: Type comparison.

July 4.

	There is a problem with type lookup:
		Which context do we start with when looking into a type?
		If we use the original context, that works, but the result is conservative, so we don't get all the benefits of path-dependent typing.
		Specifically, we don't get type polymorphism.
	Instead, we need to look up type IDs from the current context (rather than the original context).

July 5.

	What seems to be advantageous is a centralized function that converts types containing OrTypes into constructible types (if possible).
	This is not really an "approximation" of union types.
	
	The purpose of this function is to have a consistent way of determining which branch of the OrType should actually get built
	during construction.
	Rationale:
	Construction builds the lower bound of any named base type (since the lower bound is guaranteed to be compatible with the base type
	[but we don't want to build a type that's more specific than needed]). But the lower bound of a member of an intersection type
	is a union type, and the result of a type inference can be a union type, so we have to make decisions about what actually gets
	built in such cases.

	The default policy I go with here is to select the leftmost branch unless it is non-constructible.
	By choosing the leftmost branch, members of types earlier in the linearization order override members later in the order.
	The result of this policy is that declarations are narrowed from right to left, but assignments are executed from
	 left to right. This allows the most specific declarations and the earliest-executed terms to be grouped together
	 in the leftmost base type. E.g.: { T: at least A ; t = a } & { T: at least Nothing ; result = t } . new

	There's still some problems with type linearization. Fix me...!!!!

	(July 7 edit) A solution to linearization is to generate code that adds mixin initializers conditionally, so
	that all calls to a named initializer always invoke the first (leftmost) initializer. It is possible that some
	of these initializers can be made non-conditional based on statically known information, but such an
	optimization is low-priority.
	What really should be done soon is sort out the differences between type declaractions and type assignments.
	And: Bounds checking, which requires type checking.

July 6.

	Annotations... How can they be reasonably combined under union/intersection?
	If an annotation is a single bit... (can we see it from above or below? The combining rules will be different.)
	If we consider the bit 1=@readonly, and interpret & as bitwise intersection and | as bitwise union, then
		the intersection of base types is readonly only if all bases are readonly, and versa for unions.


###
